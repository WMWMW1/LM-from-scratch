{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, token_to_idx, pad_token='<pad>', unk_token='<unk>'):\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "        self.pad = pad_token\n",
    "        self.unk = unk_token\n",
    "        # 确保<pad>和<unk>标记被添加到字典中\n",
    "        if self.pad not in self.token_to_idx:\n",
    "            self.token_to_idx[self.pad] = len(self.token_to_idx)\n",
    "        if self.unk not in self.token_to_idx:\n",
    "            self.token_to_idx[self.unk] = len(self.token_to_idx)\n",
    "        \n",
    "    def encode(self, sequence):\n",
    "        # 使用.get方法返回未知字符的索引，如果字符不在字典中\n",
    "        return [self.token_to_idx.get(token, self.token_to_idx[self.unk]) for token in sequence]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        # 解码时，未知索引将被忽略或替换为<unk>\n",
    "        return [self.idx_to_token.get(idx, self.unk) for idx in indices]\n",
    "\n",
    "    \n",
    "class ExternalSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=None):\n",
    "        # 编码序列，并找出最长的序列长度\n",
    "        self.sequences = [tokenizer.encode(sequence) for sequence in sequences]\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length if max_length is not None else max(len(seq) for seq in self.sequences)\n",
    "        self.pad_idx = tokenizer.token_to_idx[tokenizer.pad]\n",
    "\n",
    "        # 对序列进行填充\n",
    "        self.sequences = [seq + [self.pad_idx] * (self.max_length - len(seq)) for seq in self.sequences]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# 模型定义\n",
    "class SimpleTransformerModel(nn.Module):\n",
    "    def __init__(self, num_tokens=4, d_model=64, nhead=2, dim_feedforward=256, num_layers=1):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        self.token_emb = nn.Embedding(num_tokens, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_tokens)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.token_emb(src)  # (Batch, Seq, Embed)\n",
    "        src = src.permute(1, 0, 2)  # (Seq, Batch, Embed) for PyTorch Transformer\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.permute(1, 0, 2)  # Back to (Batch, Seq, Embed)\n",
    "        output = self.fc(output)\n",
    "        return output[:, -1, :]  # Only return the last output\n",
    "\n",
    "# 训练函数\n",
    "def train(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in data_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)\n",
    "        loss = loss_fn(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def load_data(tokenizer):\n",
    "    raw_sequences = ['ABC', 'BCD', 'CDA', 'DAB']\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    max_length = max(len(seq) for seq in raw_sequences) - 1  # Exclude last character for labels\n",
    "\n",
    "    for seq in raw_sequences:\n",
    "        # Encode the sequence and label\n",
    "        encoded_seq = tokenizer.encode(seq[:-1]) + [tokenizer.token_to_idx['<pad>']] * (max_length - len(seq) + 1)\n",
    "        encoded_label = tokenizer.encode(seq[1:]) + [tokenizer.token_to_idx['<pad>']] * (max_length - len(seq) + 1)\n",
    "        \n",
    "        sequences.append(encoded_seq)\n",
    "        labels.append(encoded_label)\n",
    "\n",
    "    return sequences, labels\n",
    "\n",
    "\n",
    "\n",
    "def human_test_loop(model, tokenizer, device):\n",
    "    print(\"Enter a sequence of 'A', 'B', 'C', 'D' or type 'exit' to quit:\")\n",
    "    while True:\n",
    "        user_input = input(\"Input: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        input_tensor = torch.tensor([tokenizer.encode(user_input)], dtype=torch.long).to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            predicted_index = output.argmax(dim=1).item()\n",
    "            predicted_token = tokenizer.decode([predicted_index])\n",
    "            print(\"Predicted next token:\", ''.join(predicted_token))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx = {\n",
    "    'A': 0, 'B': 1, 'C': 2, 'D': 3,\n",
    "    'AND': 4, 'OR': 5, 'XOR': 6, 'NOT': 7, 'XNOR': 8, '(': 9, ')': 10\n",
    "}\n",
    "\n",
    "tokenizer = Tokenizer(token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1] [1, 2]\n",
      "['A', 'B']\n",
      "[1, 2] [2, 3]\n",
      "['B', 'C']\n",
      "[2, 3] [3, 0]\n",
      "['C', 'D']\n",
      "[3, 0] [0, 1]\n",
      "['D', 'A']\n"
     ]
    }
   ],
   "source": [
    "sequences, labels = load_data(tokenizer)\n",
    "for i in range(len(sequences)):\n",
    "    print(sequences[i],labels[i])\n",
    "    print(tokenizer.decode(sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m     human_test_loop(model, tokenizer, device)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 27\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(epoch)\n",
      "Cell \u001b[1;32mIn[20], line 69\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     68\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src)\n\u001b[1;32m---> 69\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "   \n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 定义tokenizer\n",
    "    token_to_idx = {\n",
    "    'A': 0, 'B': 1, 'C': 2, 'D': 3,\n",
    "    'AND': 4, 'OR': 5, 'XOR': 6, 'NOT': 7, 'XNOR': 8, '(': 9, ')': 10,'<pad>': 11, '<unk>': 12 \n",
    "}\n",
    "\n",
    "    tokenizer = Tokenizer(token_to_idx)\n",
    "    \n",
    "    sequences, labels = load_data(tokenizer)\n",
    "    dataset = ExternalSequenceDataset(sequences, labels, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    " \n",
    "    model = SimpleTransformerModel(num_tokens=len(tokenizer.token_to_idx)).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_idx['<pad>'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    epochs = 1\n",
    "    for epoch in range(epochs):\n",
    "        loss = train(model, dataloader, loss_fn, optimizer, device)\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss}')\n",
    "        \n",
    "        print(epoch)\n",
    "\n",
    "        \n",
    "    human_test_loop(model, tokenizer, device)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
